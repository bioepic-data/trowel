# TROWEL

![TROWEL Logo](/images/logo_1_small.png)

**TROWEL** - a Tool for Retrieving, Organizing, and Wrangling Ecological science data and Labels

TROWEL is a command-line toolkit for working with ecological science data and ontologies, particularly [BERVO](https://github.com/bioepic-data/bervo). It provides tools for retrieving datasets, extracting variables, matching terms, and analyzing term relationships using LLM embeddings.

## Features

- **Dataset Metadata Retrieval** - Fetch metadata from ESS-DIVE datasets
- **Variable Extraction** - Extract variable names and definitions from datasets
- **Term Matching** - Match terms between files using exact and fuzzy matching
- **Embedding-Based Analysis** - Analyze term relationships using LLM embeddings
- **Clustering & Visualization** - Visualize term clusters using PCA and t-SNE
- **Cross-Dataset Comparison** - Find similar terms across different ontologies and datasets

## Installation

### Requirements
- Python 3.10+
- Poetry (for dependency management)

### Quick Setup

```bash
git clone https://github.com/bioepic-data/trowel.git
cd trowel
poetry install
```

For full embedding analysis capabilities, install optional dependencies:

```bash
pip install matplotlib seaborn scikit-learn scipy
```

For DuckDB support and CurateGPT integration:

```bash
pip install duckdb curategpt
```

## Usage

### Command Structure

```bash
trowel COMMAND [OPTIONS] [ARGUMENTS]
```

All commands include help text:

```bash
trowel --help              # Show all available commands
trowel COMMAND --help      # Show help for a specific command
```

## Commands Overview

### 1. Data Download

Commands for downloading ontology and dataset data.

#### get-bervo

Download the BERVO (Biogeochemical and Ecological Processes Ontology) ontology.

```bash
trowel get-bervo -o bervo.csv
```

**Options:**
- `-o, --output TEXT` - Output file path (default: bervo.csv)

**Output:**
- CSV file containing the complete BERVO ontology with all terms, definitions, units, and category information

This command downloads the official BERVO ontology from Google Sheets and is useful for preparing the ontology for downstream analysis with other trowel commands.

### 2. ESS-DIVE Integration

Commands for working with [ESS-DIVE](https://ess-dive.lbl.gov/) environmental science datasets.

**Requires:** `ESSDIVE_TOKEN` environment variable ([get access](https://docs.ess-dive.lbl.gov/programmatic-tools/ess-dive-dataset-api#get-access))

#### get-essdive-metadata

Retrieve metadata from ESS-DIVE datasets by DOI.

```bash
trowel get-essdive-metadata \
  -p dois.txt \
  -o ./output
```

**Options:**
- `-p, --path TEXT` - Path to file with DOIs (one per line)
- `-o, --outpath TEXT` - Output directory (default: current directory)

**Output:**
- `results.tsv` - Dataset metadata
- `frequencies.txt` - Variable frequency statistics
- `filetable.tsv` - List of files from all datasets

#### get-essdive-variables

Extract variable names from dataset files and data dictionaries.

```bash
trowel get-essdive-variables \
  -p filetable.tsv \
  -o ./output \
  -w 10
```

**Options:**
- `-p, --path TEXT` - Path to filetable.tsv (auto-detected if not provided)
- `-o, --outpath TEXT` - Output directory
- `-w, --workers INTEGER` - Number of parallel workers (default: 10)

**Output:**
- `variable_names.tsv` - Extracted variable names and metadata
- `data_dictionaries.tsv` - Compiled data dictionary entries

### 3. Term Matching

Commands for matching terms between files.

#### match-term-lists

Match terms from a TSV file against a list of terms.

```bash
trowel match-term-lists \
  -t terms.tsv \
  -l target_list.txt \
  -o results.tsv \
  -f \
  -s 80
```

**Options:**
- `-t, --terms-file TEXT` - TSV file with terms in first column (required)
- `-l, --list-file TEXT` - Text file with terms, one per line (required)
- `-o, --output TEXT` - Output file path
- `-f, --fuzzy` - Enable fuzzy matching with Levenshtein distance
- `-s, --similarity-threshold FLOAT` - Similarity threshold 0-100 (default: 80)

**Output:**
TSV file with original terms plus a new column indicating matches.

### 4. Embedding-Based Analysis

Commands for analyzing term relationships using LLM embeddings. These commands work with embeddings generated by [CurateGPT](https://github.com/monarch-initiative/curategpt).

#### prepare-embeddings

Prepare a CSV file for embedding by selecting specific columns.

```bash
trowel embeddings prepare-embeddings \
  -i bervo.csv \
  -o bervo_prepared.csv \
  -c 0,1,6,12 \
  --skip-rows 1
```

**Options:**
- `-i, --input TEXT` - Input CSV file (required)
- `-o, --output TEXT` - Output CSV file (required)
- `-c, --columns TEXT` - Comma-separated column indices (0-indexed, required)
- `--skip-rows INTEGER` - Number of header rows to skip (default: 0)

#### load-embeddings

Load embeddings and compute statistics.

```bash
trowel embeddings load-embeddings \
  -e bervo_embeds.csv \
  -o ./analysis
```

**Options:**
- `-e, --embeddings TEXT` - Embedding CSV file from CurateGPT (required)
- `-o, --output TEXT` - Output directory (default: current directory)

**Output:**
- `embedding_stats.json` - Statistics (count, dimension, similarity metrics)

#### find-similar

Find the most similar terms to a query term.

```bash
trowel embeddings find-similar \
  -e bervo_embeds.csv \
  -q BERVO:0000026 \
  -n 20 \
  -o results.txt
```

**Options:**
- `-e, --embeddings TEXT` - Embedding CSV file (required)
- `-q, --query TEXT` - Query term ID or label (required)
- `-n, --top-n INTEGER` - Number of results (default: 10)
- `-o, --output TEXT` - Output file for results (optional)

#### visualize-clusters

Create 2D visualizations of term clusters.

```bash
# Fast PCA visualization
trowel embeddings visualize-clusters \
  -e bervo_embeds.csv \
  -m pca \
  -o clusters_pca.png

# Better quality t-SNE visualization
trowel embeddings visualize-clusters \
  -e bervo_embeds.csv \
  -m tsne \
  -o clusters_tsne.png
```

**Options:**
- `-e, --embeddings TEXT` - Embedding CSV file (required)
- `-m, --method [pca|tsne]` - Dimensionality reduction method (default: pca)
- `-o, --output TEXT` - Output PNG file
- `--label-interval INTEGER` - Interval for labeling points (default: 100)

#### visualize-by-category

Visualize clusters colored by ontology category.

```bash
trowel embeddings visualize-by-category \
  -s bervo.csv \
  -e bervo_embeds.csv \
  -o clusters_categorical.png
```

**Options:**
- `-s, --source-csv TEXT` - Source CSV with category information (required)
- `-e, --embeddings TEXT` - Embedding CSV file (required)
- `-o, --output TEXT` - Output PNG file
- `--label-interval INTEGER` - Interval for labeling points (default: 100)

#### visualize-heatmap

Create similarity heatmap for a subset of terms.

```bash
trowel embeddings visualize-heatmap \
  -e bervo_embeds.csv \
  -n 50 \
  -o similarity_heatmap.png
```

**Options:**
- `-e, --embeddings TEXT` - Embedding CSV file (required)
- `-n, --num-terms INTEGER` - Number of terms to include (default: 50)
- `-o, --output TEXT` - Output PNG file

#### cross-collection-similarity

Find similar term pairs between two collections.

```bash
trowel embeddings cross-collection-similarity \
  -b bervo_embeds.csv \
  -n new_vars_embeds.csv \
  -t 25 \
  -o matches.txt
```

**Options:**
- `-b, --bervo-embeddings TEXT` - First embedding file (required)
- `-n, --new-embeddings TEXT` - Second embedding file (required)
- `-t, --top-n INTEGER` - Number of top pairs (default: 25)
- `-o, --output TEXT` - Output file for results (optional)

## Common Workflows

### Workflow 1: Extract and Match Variables

```bash
# 0. Download BERVO ontology
trowel get-bervo -o bervo.csv

# 1. Get dataset metadata
trowel get-essdive-metadata -p dois.txt -o ./data

# 2. Extract variables from datasets
trowel get-essdive-variables -p ./data/filetable.tsv -o ./data

# 3. Match variables to BERVO terms
trowel match-term-lists \
  -t bervo.csv \
  -l ./data/variable_names.txt \
  -o matched_variables.tsv \
  -f
```

### Workflow 2: Analyze Term Relationships

```bash
# 1. Download BERVO ontology
trowel get-bervo -o bervo.csv

# 2. Prepare data for embedding
trowel embeddings prepare-embeddings \
  -i bervo.csv \
  -o bervo_prepared.csv \
  -c 0,1,6,12 \
  --skip-rows 1

# 3. Generate embeddings (using CurateGPT)
curategpt -vvv index \
  -c bervo \
  -D duckdb \
  --batch-size 100 \
  bervo_prepared.csv

# 4. Export embeddings to CSV
duckdb -c "SELECT id, embeddings FROM bervo" \
  db/db_file.duckdb > bervo_embeds.csv

# 5. Analyze embeddings
trowel embeddings load-embeddings \
  -e bervo_embeds.csv \
  -o ./analysis

# 6. Find similar terms
trowel embeddings find-similar \
  -e bervo_embeds.csv \
  -q BERVO:0000026 \
  -n 20 \
  -o similar_terms.txt

# 7. Create visualizations
trowel embeddings visualize-clusters \
  -e bervo_embeds.csv \
  -m pca \
  -o clusters_pca.png

trowel embeddings visualize-by-category \
  -s bervo.csv \
  -e bervo_embeds.csv \
  -o clusters_by_category.png
```

### Workflow 3: Compare Ontologies

```bash
# 1. Download BERVO for comparison (optional)
trowel get-bervo -o ontology1.csv

# 2. Prepare and embed both ontologies
trowel embeddings prepare-embeddings \
  -i ontology1.csv \
  -o ontology1_prepared.csv \
  -c 0,1,6 --skip-rows 1

trowel embeddings prepare-embeddings \
  -i ontology2.csv \
  -o ontology2_prepared.csv \
  -c 0,1,6 --skip-rows 1

# 3. Generate embeddings (using CurateGPT for each)
curategpt index -c ontology1 -D duckdb ontology1_prepared.csv
curategpt index -c ontology2 -D duckdb ontology2_prepared.csv

# 4. Export and compare
duckdb -c "SELECT id, embeddings FROM ontology1" db/db_file.duckdb > ont1_embeds.csv
duckdb -c "SELECT id, embeddings FROM ontology2" db/db_file.duckdb > ont2_embeds.csv

trowel embeddings cross-collection-similarity \
  -b ont1_embeds.csv \
  -n ont2_embeds.csv \
  -t 50 \
  -o ontology_comparison.txt
```

## About BERVO

TROWEL is specifically designed to work with the [BERVO ontology](https://github.com/bioepic-data/bervo), a comprehensive biogeochemical ontology for environmental science variables. While TROWEL may work with other ontologies, optimal results are achieved with BERVO due to its well-structured hierarchy and comprehensive variable definitions.

BERVO includes:
- Standardized variable definitions
- Clear hierarchical organization (categories starting with BERVO:9)
- Units and measurement specifications
- Integration with ESS-DIVE datasets

## Setup & Configuration

### Environment Variables

```bash
# Required for ESS-DIVE commands
export ESSDIVE_TOKEN=your_token_here

# Required for embedding generation (CurateGPT)
export OPENAI_API_KEY=your_api_key_here
```

### Getting ESS-DIVE Access

1. Visit https://docs.ess-dive.lbl.gov/programmatic-tools/ess-dive-dataset-api#get-access
2. Follow the authentication instructions
3. Set your token as shown above

### Getting OpenAI API Key

1. Sign up for an OpenAI account at https://openai.com
2. Navigate to the API keys section
3. Create a new API key
4. Set as shown above

## Requirements & Dependencies

### Core
- Python 3.10+
- click (CLI framework)
- requests (HTTP client)
- polars (data processing)
- tqdm (progress bars)
- openpyxl, xlrd (spreadsheet support)

### Optional (for embeddings)
- numpy, scipy (numerical operations)
- matplotlib, seaborn (visualization)
- scikit-learn (dimensionality reduction)
- duckdb (database access)
- curategpt (embedding generation)

Install optional dependencies:
```bash
pip install matplotlib seaborn scikit-learn scipy duckdb
```

## Troubleshooting

### "ESSDIVE_TOKEN is not set"
```bash
export ESSDIVE_TOKEN=your_token_here
```

### "Term not found" error
- Verify the exact spelling of the query term
- Check that the term exists in your embedding file

### Visualization takes too long
- Use PCA instead of t-SNE for quick previews
- Reduce the number of labeled points with `--label-interval`

### Out of memory with large datasets
- Process datasets in batches
- Use smaller subsets for visualization
- Consider using PCA for dimensionality reduction

## Contributing

Contributions are welcome! Please:
1. Fork the repository
2. Create a feature branch
3. Submit a pull request

## License

BSD-3-Clause - See LICENSE file for details

## Citation

If you use TROWEL in your research, please cite:

```bibtex
@software{trowel,
  title = {TROWEL: Tool for Retrieving, Organizing, and Wrangling Ecological Labels},
  url = {https://github.com/bioepic-data/trowel},
  year = {2024}
}
```

## Contact

For issues, questions, or suggestions, please open an issue on GitHub:
https://github.com/bioepic-data/trowel/issues

## Related Resources

- [BERVO Ontology](https://github.com/bioepic-data/bervo)
- [ESS-DIVE](https://ess-dive.lbl.gov/)
- [CurateGPT](https://github.com/monarch-initiative/curategpt)
- [OpenAI Embeddings](https://platform.openai.com/docs/guides/embeddings)